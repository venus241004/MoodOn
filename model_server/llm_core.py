# llm_core.py
"""
Qwen2.5-14B-Korean ê¸°ë°˜ LLM ëª¨ë“ˆ (8bit ì–‘ìí™” ë¡œë”©)

- ì¼ë°˜ ëŒ€í™” / ì¶”ì²œ ìƒì„±: chat_template.jinja í™œìš© (use_chat_template=True)
- JSON íŒŒì‹±(parse_user_query): í…œí”Œë¦¿ ì•ˆ ì“°ê³  ë‹¨ìˆœ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œë§Œ í˜¸ì¶œ (use_chat_template=False)
"""

import json
import re
import time
from typing import Optional, List, Dict, Any, Tuple

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from transformers.utils import logging as hf_logging

from config import HF_QWEN_MODEL_NAME
from mood_vocab import snap_moods_to_vocab, match_moods_in_text  # í…ìŠ¤íŠ¸ì—ì„œ ë¬´ë“œ íƒì§€


# =========================
# 1. ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì¶”ì²œ ì „ìš©: ê°•í•œ ë¹„í™˜ê° ê·œì¹™)
# =========================

DEFAULT_SYSTEM_PROMPT = (
    "ë„ˆëŠ” ì¸í…Œë¦¬ì–´Â·í™ˆë°ì½” ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” í•œêµ­ì–´ ì±—ë´‡ì´ë‹¤.\n"
    "ì‚¬ìš©ìì˜ ì·¨í–¥(ë¬´ë“œ, í†¤, ìŠ¤íƒ€ì¼), ì˜ˆì‚°, ê³µê°„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒí’ˆì„ ì¶”ì²œí•˜ê±°ë‚˜ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•œë‹¤.\n\n"
    "[ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ â€“ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•¨]\n"
    "1) ì‹œìŠ¤í…œì´ ì œê³µí•œ í›„ë³´ ìƒí’ˆ ëª©ë¡(ë¸Œëœë“œ, ìƒí’ˆëª…, ê°€ê²©, ë§í¬) ë°–ì˜ ìƒí’ˆì„ ë§Œë“¤ì§€ ì•ŠëŠ”ë‹¤.\n"
    "2) ë¸Œëœë“œ/ìƒí’ˆëª…/ê°€ê²©/ë§í¬ëŠ” ì…ë ¥ìœ¼ë¡œ ë°›ì€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.\n"
    "3) í›„ë³´ê°€ ì—†ìœ¼ë©´ ì†”ì§íˆ 'í˜„ì¬ ë°ì´í„°ì— ì—†ì–´ìš”'ë¼ê³  ë§í•œë‹¤. ìƒìƒìœ¼ë¡œ ì±„ìš°ì§€ ì•ŠëŠ”ë‹¤.\n"
    "4) ê°œë…ì  ì¡°ì–¸ì€ í•´ë„ ë˜ì§€ë§Œ, ê·¸ ê²½ìš° ë¸Œëœë“œ/ìƒí’ˆëª…/ê°€ê²©/ë§í¬ë¥¼ ë¶™ì´ì§€ ì•ŠëŠ”ë‹¤.\n\n"
    "[ë‹µë³€ ìŠ¤íƒ€ì¼]\n"
    "- í†¤: ë”°ëœ»í•œ ì¡´ëŒ“ë§, ì¼ìƒ ëŒ€í™”ì²´. ê¸ˆì§€ì–´: ê·€í•˜, ê³ ê°ë‹˜, ì¹œì• í•˜ëŠ”, ë‹¹ì‹ , ~ë‹˜.\n"
    "- í˜¸ì¹­: ì´ë¦„/í˜¸ì¹­ì„ ë¶™ì´ì§€ ë§ê³  ë°”ë¡œ ë§í•˜ê¸°(ì˜ˆ: 'ì´ ë°©ì€ ...', 'ì´ëŸ° ë¬´ë“œê°€ ì˜ ì–´ìš¸ë ¤ìš”').\n"
    "- ê¸¸ì´: 3~6ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ. ë¶ˆë¦¿ì´ë‚˜ ë¬¸ì¥ ìœ„ì£¼ë¡œ í•µì‹¬ë§Œ.\n"
    "- ì‹œì‘: ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ê·¸ ë¶„ìœ„ê¸°ë¥¼ ì§§ê²Œ ì–¸ê¸‰ í›„ ì œì•ˆìœ¼ë¡œ ì´ì–´ê°€ê¸°.\n"
    "- êµ¬ì¡°: í˜„ì¬ ê³µê°„/ë¬´ë“œ ìš”ì•½ â†’ ëª©í‘œ ë¬´ë“œ/ìš”ì²­ ë°˜ì˜ â†’ í›„ë³´ ìƒí’ˆ/ì•„ì´ë””ì–´ ì œì•ˆ.\n"
)


# =========================
# 2. ëª¨ë¸ ë¡œë”© (8bit)
# =========================

hf_logging.set_verbosity_error()

print(f"[LLM] â–¶ Qwen2.5-14B-Korean (8bit, device_map='auto') ë¡œë”© ì¤‘...")

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
)

tokenizer = AutoTokenizer.from_pretrained(
    HF_QWEN_MODEL_NAME,
    use_fast=True,
    trust_remote_code=True,
)

model = AutoModelForCausalLM.from_pretrained(
    HF_QWEN_MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

model.eval()


# =========================
# 3. ì¶œë ¥ í›„ì²˜ë¦¬
# =========================

def clean_trailing_incomplete_sentence(text: str) -> str:
    """
    ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ì¤‘ê°„ì— ëŠê¸´ ê²½ìš°,
    ë§ˆì§€ë§‰ 'ì™„ì „í•œ ë¬¸ì¥'ê¹Œì§€ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ ì˜ë¼ë‚¸ë‹¤.
    """
    text = text.strip()
    if len(text) < 40:
        return text

    length = len(text)
    best_cut = -1

    # 1) êµ¬ë‘ì  ê¸°ì¤€
    enders = [".", "?", "!", "â€¦", "ã€‚", "ï¼", "ï¼Ÿ"]
    last_punc_idx = -1
    for ch in enders:
        idx = text.rfind(ch)
        if idx > last_punc_idx:
            last_punc_idx = idx

    if last_punc_idx != -1 and last_punc_idx > length * 0.3:
        best_cut = max(best_cut, last_punc_idx + 1)

    # 2) í•œêµ­ì–´ ì¢…ê²° ì–´ë¯¸ ê¸°ì¤€
    ender_pattern = re.compile(
        r"(ìš”|ì…ë‹ˆë‹¤|í•©ë‹ˆë‹¤|ì˜ˆìš”|ì—ìš”|ê±°ì˜ˆìš”|ê±°ì—ìš”)(?=[^ê°€-í£]|$)"
    )

    last_match_end = -1
    for m in ender_pattern.finditer(text):
        end_pos = m.end(1)
        if end_pos > last_match_end:
            last_match_end = end_pos

    if last_match_end != -1 and last_match_end > length * 0.3:
        best_cut = max(best_cut, last_match_end)

    if best_cut == -1:
        return text

    cleaned = text[:best_cut].strip()
    if len(cleaned) < length * 0.5:
        return text

    return cleaned


# =========================
# 4. ì…ë ¥ ë¹Œë”
# =========================

def _build_inputs_with_template(messages: List[Dict[str, str]]):
    """Qwen chat_template.jinja ë¥¼ ì‚¬ìš©í•œ ì…ë ¥ ìƒì„± (ì¼ë°˜ ëŒ€í™”ìš©)."""
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    )
    return {"input_ids": input_ids}


def _build_inputs_fallback(system_prompt: str, user_text: str):
    """
    chat_template ì—†ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì…ë ¥ ìƒì„±
    (parse_user_queryìš©: ë²„ê·¸ íšŒí”¼ìš©)
    """
    text = (
        f"[SYSTEM]\n{system_prompt}\n\n"
        f"[USER]\n{user_text}\n\n"
        "[ASSISTANT]\n"
    )
    enc = tokenizer(text, return_tensors="pt")
    return {"input_ids": enc["input_ids"]}


# =========================
# 5. ê³µí†µ chat í•¨ìˆ˜
# =========================

def chat(
    history: List[Tuple[str, str]],
    user_input: str,
    system_prompt: str = DEFAULT_SYSTEM_PROMPT,
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    do_sample: bool = True,
    use_chat_template: bool = True,
) -> str:
    """
    history: [(user, assistant), ...]
    user_input: ì´ë²ˆ í„´ ì‚¬ìš©ì ì…ë ¥ (í˜¹ì€ RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸)

    - use_chat_template=True  â†’ Qwen chat_template ì‚¬ìš© (ì¼ë°˜ ëŒ€í™”/ì¶”ì²œ)
    - use_chat_template=False â†’ fallback í…ìŠ¤íŠ¸ í¬ë§· ì‚¬ìš© (íŒŒì„œ)

    âš ï¸ ì£¼ì˜:
    - main.pyì—ì„œ ì¶”ì²œ ëª¨ë“œ(handle_recommend)ëŠ” system_promptë¡œ DEFAULT_SYSTEM_PROMPTë¥¼ ë„˜ê¸´ë‹¤.
      ì´ ê²½ìš°ë¥¼ 'ìƒí’ˆ ì¶”ì²œ/ë¹„í™˜ê° ëª¨ë“œ'ë¡œ ê°„ì£¼í•˜ì—¬ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •í•œë‹¤.
    """
    # ì¶”ì²œ ëª¨ë“œ ì—¬ë¶€ íŒë‹¨ (DEFAULT_SYSTEM_PROMPT ê·¸ëŒ€ë¡œ ì“´ ê²½ìš°)
    is_recommendation_mode = (system_prompt == DEFAULT_SYSTEM_PROMPT)

    # ì¶”ì²œ ëª¨ë“œì—ì„œëŠ” ì˜¨ë„ë¥¼ ì¡°ê¸ˆ ë‚®ì¶”ê³ (top_pë„ ì‚´ì§ ë‚®ì¶¤) ë” ë³´ìˆ˜ì ìœ¼ë¡œ ë‹µë³€
    effective_temperature = temperature
    effective_top_p = top_p
    if is_recommendation_mode:
        # ì§€ë‚˜ì¹˜ê²Œ ì°½ì˜ì ì¸(=í™˜ê°) ì¶œë ¥ì„ ì¤„ì´ê¸° ìœ„í•´ ìƒí•œì„ ë‘”ë‹¤.
        effective_temperature = min(float(temperature), 0.5)
        effective_top_p = min(float(top_p), 0.85)

    if use_chat_template:
        messages: List[Dict[str, str]] = []
        messages.append({"role": "system", "content": system_prompt})

        for q, a in history:
            messages.append({"role": "user", "content": q})
            messages.append({"role": "assistant", "content": a})

        messages.append({"role": "user", "content": user_input})
        inputs = _build_inputs_with_template(messages)
    else:
        inputs = _build_inputs_fallback(system_prompt, user_input)

    input_ids = inputs["input_ids"]
    attention_mask = torch.ones_like(input_ids)

    main_device = next(model.parameters()).device
    input_ids = input_ids.to(main_device)
    attention_mask = attention_mask.to(main_device)

    gen_kwargs = dict(
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
    )

    if do_sample:
        gen_kwargs.update(
            do_sample=True,
            temperature=float(effective_temperature),
            top_p=float(effective_top_p),
        )
    else:
        gen_kwargs.update(do_sample=False)

    t0 = time.time()
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **gen_kwargs,
        )
    t1 = time.time()
    _elapsed = t1 - t0  # ë‚´ë¶€ì—ì„œëŠ” ë¡œê·¸ë§Œ ì œê±°, ê°’ì€ í•„ìš”í•˜ë©´ ë””ë²„ê·¸ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘˜ ìˆ˜ ìˆìŒ

    input_len = input_ids.shape[1]
    generated_ids = outputs[0][input_len:]

    text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    text = text.strip()
    text = clean_trailing_incomplete_sentence(text)

    # ğŸ”‡ ì—¬ê¸°ì„œëŠ” ì‹œê°„ ë¡œê·¸ ì¶œë ¥ ì•ˆ í•¨ (main.pyì—ì„œë§Œ ì¶œë ¥)
    return text


# =========================
# 6. ì‚¬ìš©ì ì§ˆì˜ íŒŒì‹± (ì¹´í…Œê³ ë¦¬/ë¬´ë“œ/ì˜ˆì‚°/ê³µê°„)
# =========================

def parse_user_query(user_text: str) -> Dict[str, Any]:
    """
    Qwenì—ê²Œ í•œ ë²ˆ ë¬¼ì–´ì„œ:
    - category: "ëŸ¬ê·¸", "ì»¤íŠ¼", "ì¡°ëª…", "ìˆ˜ë‚©ì¥" ë“± (ì—†ìœ¼ë©´ null)
    - price_min / price_max: ì› ë‹¨ìœ„ ì •ìˆ˜ (ì—†ìœ¼ë©´ null)
    - moods: ["ì•„ëŠ‘í•œ", "ìš°ë“œí†¤", "ëª¨ë˜", ...] ë¦¬ìŠ¤íŠ¸
    - space: "ì¹¨ì‹¤", "ê±°ì‹¤", "ì±…ìƒ ê·¼ì²˜" ë“±

    + ì—¬ëŸ¬ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë³´ì • (ì˜ˆì‚°/ê³µê°„/ë¬´ë“œ/ì¹´í…Œê³ ë¦¬).
    """

    # ---------- 0) íœ´ë¦¬ìŠ¤í‹±ë“¤ ----------

    def _heuristic_detect_space(text: str) -> Optional[str]:
        if "ì¹¨ì‹¤" in text:
            return "ì¹¨ì‹¤"
        if "ê±°ì‹¤" in text:
            return "ê±°ì‹¤"
        if "ì‘ì—…ì‹¤" in text or "ê³µë¶€ë°©" in text or "ì„œì¬" in text:
            return "ì‘ì—…ì‹¤/ì„œì¬"
        if "ì±…ìƒ" in text:
            return "ì±…ìƒ ê·¼ì²˜"
        if "ì£¼ë°©" in text or "ë¶€ì—Œ" in text:
            return "ì£¼ë°©"
        return None

    def _heuristic_detect_moods(text: str) -> List[str]:
        candidates: List[str] = []

        mood_patterns = [
            ("ì°¨ë¶„", "ì°¨ë¶„í•œ"),
            ("ì”ì”", "ì°¨ë¶„í•œ"),
            ("ë”°ëœ»", "ë”°ëœ»í•œ"),
            ("í¬ê·¼", "í¬ê·¼í•œ"),
            ("ì•„ëŠ‘", "ì•„ëŠ‘í•œ"),
            ("í¸ì•ˆ", "í¸ì•ˆí•œ"),
            ("ëª¨ë˜", "ëª¨ë˜"),
            ("í˜„ëŒ€ì ", "ëª¨ë˜"),
            ("ì‹¬í”Œ", "ë¯¸ë‹ˆë©€"),
            ("ë¯¸ë‹ˆë©€", "ë¯¸ë‹ˆë©€"),
            ("ë¶ìœ ëŸ½", "ë¶ìœ ëŸ½í’"),
            ("í˜¸í…”", "í˜¸í…”ì‹"),
            ("ìš°ë“œí†¤", "ìš°ë“œí†¤"),
            ("í™”ì´íŠ¸í†¤", "í™”ì´íŠ¸í†¤"),
        ]

        for key, label in mood_patterns:
            if key in text:
                candidates.append(label)

        return list(dict.fromkeys(candidates))

    def _heuristic_detect_budget(text: str) -> Tuple[Optional[int], Optional[int]]:
        """
        ì˜ˆì‚° ê´€ë ¨ íœ´ë¦¬ìŠ¤í‹± íŒŒì„œ.
        """
        m_range2 = re.search(
            r"(\d+)\s*ë§Œ\s*ì›?\s*(?:ì´ìƒ|ì´ˆê³¼|ë¶€í„°)[^0-9]{0,15}(\d+)\s*ë§Œ\s*ì›?\s*(?:ì´í•˜|ì´ë‚´|ê¹Œì§€|ì–¸ë”|ì•„ë˜|ë°‘)",
            text,
        )
        if m_range2:
            a = int(m_range2.group(1)) * 10000
            b = int(m_range2.group(2)) * 10000
            return (min(a, b), max(a, b))

        m_range = re.search(
            r"(\d+)\s*ë§Œ\s*ì›?\s*(?:ì—ì„œ|~|-)\s*(\d+)\s*ë§Œ",
            text,
        )
        if m_range:
            a = int(m_range.group(1)) * 10000
            b = int(m_range.group(2)) * 10000
            return (min(a, b), max(a, b))

        m_around_num = re.search(
            r"(\d+)\s*ë§Œ\s*ì›?\s*(?:ì •ë„|ì¯¤|ì „í›„|ê·¼ì²˜|ê·¼ë°©|ì–¸ì €ë¦¬)",
            text,
        )
        if m_around_num:
            v = int(m_around_num.group(1)) * 10000
            lo = int(v * 0.8)
            hi = int(v * 1.2)
            return (lo, hi)

        matches = [
            (int(m.group(1)) * 10000, m.start())
            for m in re.finditer(r"(\d+)\s*ë§Œ\s*ì›?", text)
        ]
        if not matches:
            return (None, None)

        cap_words = ["ì´ë‚´", "ì´í•˜", "ê¹Œì§€", "ìµœëŒ€", "ì–¸ë”", "ì•„ë˜", "ë°‘"]
        low_words = ["ì´ìƒ", "ë¶€í„°", "ë„˜ê²Œ", "ì´ˆê³¼", "ì˜¤ë²„", "ìœ„"]
        around_words = ["ì •ë„", "ì¯¤", "ì „í›„", "ê·¼ì²˜", "ê·¼ë°©", "ì–¸ì €ë¦¬"]

        def first_pos(words: List[str]) -> int:
            poss = [text.find(w) for w in words if w in text]
            return min(poss) if poss else -1

        cap_pos = first_pos(cap_words)
        low_pos = first_pos(low_words)
        around_pos = first_pos(around_words)

        def pick_before(pos: int) -> int:
            if pos == -1:
                return matches[-1][0]
            before = [m for m in matches if m[1] <= pos]
            return before[-1][0] if before else matches[-1][0]

        if cap_pos != -1:
            v = pick_before(cap_pos)
            return (None, v)

        if low_pos != -1:
            v = pick_before(low_pos)
            return (v, None)

        v = matches[-1][0]
        if around_pos != -1:
            lo = int(v * 0.8)
            hi = int(v * 1.2)
            return (lo, hi)

        lo = int(v * 0.7)
        hi = int(v * 1.3)
        return (lo, hi)

    def _looks_like_interior_context(text: str) -> bool:
        interior_words = [
            "ì¸í…Œë¦¬ì–´", "ì§‘ ê¾¸ë¯¸", "ì§‘ê¾¸ë¯¸",
            "ê³µê°„", "ë°©", "ê±°ì‹¤", "ì¹¨ì‹¤", "ì‘ì—…ì‹¤", "ì„œì¬",
            "ê°€êµ¬", "ì†Œí’ˆ", "ì¿ ì…˜", "ëŸ¬ê·¸", "ì¡°ëª…", "ì»¤íŠ¼",
        ]
        return any(w in text for w in interior_words)

    def _heuristic_detect_category(text: str) -> Optional[str]:
        t = text.replace(" ", "").lower()

        if any(k in t for k in ["ì¡°ëª…", "ë¨í”„", "ë¬´ë“œë“±", "ìŠ¤íƒ ë“œ", "ë²½ê±¸ì´ì¡°ëª…", "ë²½ë“±", "ë°±ì—´ë“±"]):
            return "ì¡°ëª…"
        if any(k in t for k in ["ëŸ¬ê·¸", "ì¹´í˜íŠ¸", "ì¹´í«", "ì¹´í«íŠ¸"]):
            return "ëŸ¬ê·¸_ì»¤íŠ¼"
        if any(k in t for k in ["ì»¤íŠ¼", "ë¸”ë¼ì¸ë“œ"]):
            return "ëŸ¬ê·¸_ì»¤íŠ¼"
        if any(k in t for k in ["ì¿ ì…˜", "ì¿ ì…˜ì»¤ë²„", "ë°©ì„"]):
            return "ì¿ ì…˜"
        if any(k in t for k in ["ì´ë¶ˆ", "ì¹¨êµ¬", "ë² ë”©", "ì´ë¶ˆì»¤ë²„", "ì¹¨ëŒ€ì»¤ë²„"]):
            return "ì¹¨êµ¬"
        if any(k in t for k in ["ì„ ë°˜", "ìˆ˜ë‚©", "ì„œë", "ì±…ì¥", "ìˆ˜ë‚©ì¥"]):
            return "ìˆ˜ë‚©ì •ë¦¬"
        return None

    def _normalize_category_str(cat: Optional[str]) -> Optional[str]:
        if not cat:
            return None
        s = str(cat).strip().lower()

        mapping = {
            "lighting": "ì¡°ëª…",
            "light": "ì¡°ëª…",
            "lamp": "ì¡°ëª…",
            "rug": "ëŸ¬ê·¸_ì»¤íŠ¼",
            "curtain": "ëŸ¬ê·¸_ì»¤íŠ¼",
            "carpet": "ëŸ¬ê·¸_ì»¤íŠ¼",
            "bedding": "ì¹¨êµ¬",
            "blanket": "ì¹¨êµ¬",
            "duvet": "ì¹¨êµ¬",
            "pillow": "ì¿ ì…˜",
            "cushion": "ì¿ ì…˜",
            "storage": "ìˆ˜ë‚©ì •ë¦¬",
            "shelf": "ìˆ˜ë‚©ì •ë¦¬",
        }

        if s in mapping:
            return mapping[s]

        if re.search(r"[ê°€-í£]", s):
            return s
        return s

    # ---------- 1) LLM ê¸°ë°˜ 1ì°¨ íŒŒì‹± ----------

    parse_system_prompt = (
        "ë„ˆëŠ” ì¸í…Œë¦¬ì–´ ìƒí’ˆ ì¶”ì²œ ì‹œìŠ¤í…œì˜ íŒŒì„œ(parser)ì´ë‹¤. "
        "ì‚¬ìš©ìì˜ í•œêµ­ì–´ ë¬¸ì¥ì„ ì½ê³  ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶”ì¶œí•´ë¼.\n\n"
        'í•„ë“œ ì„¤ëª…:\n'
        '  - "category": ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì£¼ìš” ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "ëŸ¬ê·¸", "ì»¤íŠ¼", "ì¡°ëª…", "ìˆ˜ë‚©ì¥"). ì—†ìœ¼ë©´ null.\n'
        '  - "price_min": ì˜ˆì‚°ì˜ ìµœì†Œê°’ (ì› ë‹¨ìœ„ ì •ìˆ˜). ì—†ìœ¼ë©´ null.\n'
        '  - "price_max": ì˜ˆì‚°ì˜ ìµœëŒ€ê°’ (ì› ë‹¨ìœ„ ì •ìˆ˜). ì—†ìœ¼ë©´ null.\n'
        '  - "moods": ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë¬´ë“œ/ë¶„ìœ„ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•œêµ­ì–´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸.\n'
        '  - "space": ì‚¬ìš©ìê°€ ê¾¸ë¯¸ê³  ì‹¶ë‹¤ê³  ë§í•œ ì£¼ìš” ê³µê°„. ì˜ˆ: "ì±…ìƒ ê·¼ì²˜", "ì¹¨ì‹¤", "ê±°ì‹¤", "ì‘ì—…ì‹¤" ë“±.\n\n'
        "ì¤‘ìš” ê·œì¹™:\n"
        "1) ë¬´ë“œ(moods)ì—ëŠ” ë¶„ìœ„ê¸°/ìŠ¤íƒ€ì¼ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ë§Œ ë„£ì–´ë¼.\n"
        "2) 'ì±…ìƒ ê·¼ì²˜', 'ì¹¨ì‹¤', 'ê±°ì‹¤' ê°™ì€ ê³µê°„ í‘œí˜„ì€ spaceì—ë§Œ ë„£ê³  moodsì—ëŠ” ë„£ì§€ ë§ˆë¼.\n"
        "3) ì˜ˆì‚°ì´ ì „í˜€ ì–¸ê¸‰ë˜ì§€ ì•Šìœ¼ë©´ price_min, price_maxëŠ” ëª¨ë‘ nullë¡œ ë‘”ë‹¤.\n"
        "4) JSON ì´ì™¸ì˜ ê¸€ìëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆë¼."
    )

    parse_user_prompt = (
        f"ì‚¬ìš©ì ì…ë ¥: {user_text}\n\n"
        "ìœ„ ì„¤ëª…ëŒ€ë¡œ JSONë§Œ ì¶œë ¥í•´."
    )

    raw = chat(
        history=[],
        user_input=parse_user_prompt,
        system_prompt=parse_system_prompt,
        max_new_tokens=256,
        temperature=0.0,
        top_p=1.0,
        do_sample=False,
        use_chat_template=False,
    )

    # ---------- 2) JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ ----------

    match = re.search(r"\{.*\}", raw, re.DOTALL)
    if not match:
        data = {}
    else:
        json_str = match.group(0)
        try:
            data = json.loads(json_str)
        except Exception:
            data = {}

    # ---------- 3) 1ì°¨ ì¶”ì¶œ ê°’ ----------

    category = data.get("category")
    price_min = data.get("price_min")
    price_max = data.get("price_max")
    moods = data.get("moods") or []
    space = data.get("space")

    # ---------- 4) íƒ€ì…/í¬ë§· ì •ë¦¬ ----------

    if isinstance(moods, str):
        moods = [m.strip() for m in moods.split(",") if m.strip()]
    elif isinstance(moods, list):
        moods = [str(m).strip() for m in moods if str(m).strip()]
    else:
        moods = []

    def _to_int_or_none(x):
        if x is None:
            return None
        if isinstance(x, (int, float)):
            return int(x)
        s = str(x).replace(",", "").replace(" ", "")
        if s.isdigit():
            return int(s)
        return None

    price_min = _to_int_or_none(price_min)
    price_max = _to_int_or_none(price_max)

    text = user_text.strip()

    # ---------- 4-1) ë¬´ë“œ ì‚¬ì „ì— ê¸°ë°˜í•œ ì§ì ‘ íƒì§€ ----------

    # ì‚¬ìš©ìì˜ ì›ë¬¸ ë¬¸ì¥ì—ì„œ ì‚¬ì „ì— ìˆëŠ” ë¬´ë“œ í‚¤ì›Œë“œë¥¼ ì§ì ‘ ì°¾ì•„ì„œ
    # LLMì´ ë½‘ì€ moods ë¦¬ìŠ¤íŠ¸ì— í•©ì³ì¤€ë‹¤.
    detected_moods_from_text = match_moods_in_text(text)
    for m in detected_moods_from_text:
        if m not in moods:
            moods.append(m)

    # ---------- 5) ì˜ˆì‚° íœ´ë¦¬ìŠ¤í‹±: ë‘˜ ë‹¤ Noneì¼ ë•Œë§Œ ----------

    if price_min is None and price_max is None:
        h_min, h_max = _heuristic_detect_budget(text)
        if h_min is not None or h_max is not None:
            price_min, price_max = h_min, h_max

    # ---------- 6) ì¸í…Œë¦¬ì–´ ë¬¸ë§¥ì¼ ë•Œ space/moods ë³´ì • ----------

    if _looks_like_interior_context(text):
        if space is None:
            h_space = _heuristic_detect_space(text)
            if h_space:
                space = h_space

        # LLM/ì‚¬ì „ ë‘˜ ë‹¤ ëª» ì¡ì•˜ì„ ë•Œë§Œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì±„ì›€
        if not moods:
            h_moods = _heuristic_detect_moods(text)
            if h_moods:
                moods = h_moods

    # ---------- 7) ë¬´ë“œ ì •ê·œí™” + 'ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬´ë“œ' ê²€ì¶œ ----------

    canonical_moods, unknown_moods = snap_moods_to_vocab(moods)

    # ğŸ”¹ ë¬´ë“œë¡œ ì“°ë©´ ì•ˆ ë˜ëŠ” ë‹¨ì–´ë“¤ ì •ë¦¬ (ì‚¬ì§„ì€ ì´ë¯¸ì§€ë¥¼ ê°€ë¦¬í‚¤ëŠ” ë§ì¼ ë¿)
    BAD_MOOD_TOKENS = {"ì†Œí’ˆ", "ì‚¬ì§„", "ì‚¬ì§„ê°™ì€", "ì‚¬ì§„ ê°™ì€", "ì´ë¯¸ì§€", "ê·¸ë¦¼"}

    # ğŸ”¹ canonical ë¬´ë“œë§Œ ì§„ì§œ moodsë¡œ ì¸ì • + BAD_MOOD_TOKENS ì œê±°
    moods = [m for m in canonical_moods if m not in BAD_MOOD_TOKENS]
    unknown_moods = [m for m in unknown_moods if m not in BAD_MOOD_TOKENS]

    # ---------- 8) ì¹´í…Œê³ ë¦¬ ë³´ì • ----------

    category = _normalize_category_str(category)

    if category is None:
        h_cat = _heuristic_detect_category(text)
        if h_cat:
            category = h_cat

    return {
        "category": category or None,
        "price_min": price_min,
        "price_max": price_max,
        "moods": moods,
        "space": space or None,
        # ì‚¬ì „ì— ì—†ëŠ” ë¬´ë“œ í‘œí˜„ ë¦¬ìŠ¤íŠ¸
        "unknown_moods": unknown_moods,
    }
