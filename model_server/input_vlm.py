# input_vlm.py
"""
Qwen2.5-VL-7B-Instruct ê¸°ë°˜ VLM ëª¨ë“ˆ

- ëª¨ë¸: Qwen/Qwen2.5-VL-7B-Instruct
- ìš©ë„: ë°©/ì¸í…Œë¦¬ì–´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ê³µê°„/ë¬´ë“œ/ì»¬ëŸ¬/ì¬ì§ˆ ë“±ì„ ì¶”ì¶œí•´ì„œ
        RAG ì¶”ì²œ ì±—ë´‡ì— ë„˜ê¸¸ êµ¬ì¡°í™”ëœ JSONì„ ìƒì„±

ì¶”ê°€ ê¸°ëŠ¥:
- ë„ˆë¬´ íë¦¬ê±°ë‚˜ ì–´ë‘ìš´ ì‚¬ì§„ ì‚¬ì „ ì°¨ë‹¨
- ì¸í…Œë¦¬ì–´ ê³µê°„ì´ ì•„ë‹Œ ì‚¬ì§„(ì¸ë¬¼/ë°˜ë ¤ë™ë¬¼/í’ê²½/ì œí’ˆ í´ë¡œì¦ˆì—… ë“±) ì°¨ë‹¨

í•„ìˆ˜:
    pip install qwen-vl-utils
    pip install numpy pillow
    (transformers 5.0.0.dev0 ê¸°ì¤€)

config.py ì—ëŠ” ìµœì†Œí•œ ì•„ë˜ ê°’ì´ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨:
    VLM_MODEL_NAME = "Qwen/Qwen2.5-VL-7B-Instruct"
(ì´ë¯¸ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
"""

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import torch
from transformers import AutoProcessor
from transformers import Qwen2_5_VLForConditionalGeneration
from qwen_vl_utils import process_vision_info

from config import VLM_MODEL_NAME  # ì˜ˆ: "Qwen/Qwen2.5-VL-7B-Instruct"

# ğŸ”¹ ì¶”ê°€: í’ˆì§ˆ ê²€ì‚¬ìš©
import numpy as np
from PIL import Image


# =========================
# ë‚´ë¶€ ì„¤ì •
# =========================

# í•´ìƒë„ ì œí•œ (í”½ì…€ ìˆ˜ ê¸°ì¤€) â†’ VRAM ì ˆì•½ìš©
#   28*28 ë‹¨ìœ„ë¡œ í”½ì…€ ìˆ˜ë¥¼ ë§ì¶”ëŠ”ê²Œ ê³µì‹ ê¶Œì¥ ë°©ì‹
MIN_PIXELS = 256 * 28 * 28   # ë„ˆë¬´ ì‘ì€ ì‚¬ì§„ ë°©ì§€
MAX_PIXELS = 768 * 28 * 28   # 1024*28*28 ì •ë„ê¹Œì§€ ì˜¬ë ¤ë„ ë˜ì§€ë§Œ VRAMì— ë”°ë¼ ì¡°ì ˆ

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32


SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì¸í…Œë¦¬ì–´ ìŠ¤íƒ€ì¼ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì˜¬ë¦° ë°©/ì¸í…Œë¦¬ì–´ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì•„ë˜ í•­ëª©ì„ **ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ** ì¶œë ¥í•˜ì„¸ìš”.

ì¶œë ¥ JSON ìŠ¤í‚¤ë§ˆ (í‚¤ ì´ë¦„/íƒ€ì…ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”):

{
  "image_type": "ì´ë¯¸ì§€ ì¢…ë¥˜. ì•„ë˜ ì¤‘ í•˜ë‚˜: 'interior_room', 'human', 'pet', 'landscape', 'object_closeup', 'document', 'other'",
  "image_type_detail": "ì§§ì€ í•œêµ­ì–´ ì„¤ëª… (ì˜ˆ: 'ì¸ë¬¼ ì…€ì¹´', 'ê³ ì–‘ì´ í´ë¡œì¦ˆì—…', 'ê±°ì‹¤ ì¸í…Œë¦¬ì–´', 'ì œí’ˆ ìƒì„¸ìƒ·' ë“±)",
  "space_ko": "ë¬¸ìì—´, í•œêµ­ì–´ë¡œ ê³µê°„ ì„¤ëª… (ì˜ˆ: 'ë”°ëœ»í•œ ëŠë‚Œì˜ ì›ë£¸ ê±°ì‹¤')",
  "space_en": "ë¬¸ìì—´, ì˜ì–´ë¡œ ì§§ì€ ê³µê°„ íƒ€ì… (ì˜ˆ: 'living room', 'bedroom', 'home office')",
  "style_keywords": ["ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œ í•œêµ­ì–´", ...],
  "color_keywords": ["ìƒ‰ê°/í†¤ í‚¤ì›Œë“œ í•œêµ­ì–´", ...],
  "mood_keywords": ["ê°ì„±/ë¶„ìœ„ê¸° í‚¤ì›Œë“œ í•œêµ­ì–´", ...],
  "material_keywords": ["ì¬ì§ˆ/ë§ˆê° í‚¤ì›Œë“œ í•œêµ­ì–´", ...],
  "lighting_keywords": ["ì¡°ëª… ê´€ë ¨ í‚¤ì›Œë“œ í•œêµ­ì–´", ...],
  "overall_comment_ko": "í•œë‘ ë¬¸ì¥ ì •ë„ì˜ í•œêµ­ì–´ ìš”ì•½ ì½”ë©˜íŠ¸"
}

image_type ë¶„ë¥˜ ê·œì¹™:
- 'interior_room' : ë°©/ê±°ì‹¤/ì£¼ë°©/ì‘ì—…ì‹¤ ë“±, ì‹¤ë‚´ ì¸í…Œë¦¬ì–´ê°€ í™”ë©´ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•˜ëŠ” ê²½ìš°
- 'human' : ì‚¬ëŒ(ì–¼êµ´/ìƒë°˜ì‹ /ì „ì‹ )ì´ í™”ë©´ì—ì„œ ê°€ì¥ ëˆˆì— ë„ëŠ” ê²½ìš° (ì…€ì¹´, í”„ë¡œí•„ ì‚¬ì§„ ë“±)
- 'pet' : ê³ ì–‘ì´, ê°•ì•„ì§€ ë“± ë°˜ë ¤ë™ë¬¼ì´ í™”ë©´ì˜ ì£¼ì¸ê³µì¸ ê²½ìš°
- 'landscape' : ìì—° í’ê²½, ë„ì‹œ ì•¼ê²½, ë°”ë‹¤, ì‚° ë“± ì‹¤ë‚´ê°€ ì•„ë‹Œ í’ê²½ ì‚¬ì§„
- 'object_closeup' : ì œí’ˆ í•˜ë‚˜, ê°€êµ¬ ì¼ë¶€, ì‘ì€ ë¬¼ê±´ì´ í™”ë©´ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•˜ëŠ” í´ë¡œì¦ˆì—… ìƒ·
- 'document' : ë¬¸ì„œ/í™”ë©´ ìº¡ì²˜/í…ìŠ¤íŠ¸ê°€ ì¤‘ì‹¬ì¸ ì´ë¯¸ì§€
- 'other' : ìœ„ ì–´ëŠ ìª½ì—ë„ ëšœë ·í•˜ê²Œ ì†í•˜ì§€ ì•ŠëŠ” ê²½ìš°

[í‚¤ì›Œë“œ ì‘ì„± ê·œì¹™ â€“ ë§¤ìš° ì¤‘ìš”]
- style_keywords, color_keywords, mood_keywords, material_keywords, lighting_keywords í•­ëª©ì— ë“¤ì–´ê°€ëŠ”
  ëª¨ë“  ë¬¸ìì—´ì€ ë°˜ë“œì‹œ **ìˆœìˆ˜ í•œêµ­ì–´ ë‹¨ì–´**ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
- ì˜ì–´ ë‹¨ì–´, ì•ŒíŒŒë²³, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì, ì–¸ë”ìŠ¤ì½”ì–´("_")ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì˜ëœ ì˜ˆì‹œ:
  - "mood_keywords": ["ë”°ëœ»í•œ", "í¸ì•ˆí•œ", "í¬ê·¼í•œ"]
  - "color_keywords": ["ë² ì´ì§€ í†¤", "ìš°ë“œí†¤", "ë¸Œë¼ìš´ í†¤"]
  - "material_keywords": ["ì½”íŠ¼", "ë²¨ë²³", "ë¦°ë„¨"]
  - "lighting_keywords": ["ì€ì€í•œ ì¡°ëª…", "ë…¸ë€ ì¡°ëª…"]
- ì˜ëª»ëœ ì˜ˆì‹œ (ì ˆëŒ€ë¡œ ì‚¬ìš© ê¸ˆì§€):
  - ["warm", "comfortable", "relaxing"]
  - ["_beige", "beige", "brown"]
  - ["cotton", "velvet", "wool"]
  - ["natural light", "soft lighting"]
- ì˜ì–´ë¡œ ë¨¼ì € ë– ì˜¬ëë‹¤ë©´, ë°˜ë“œì‹œ ê·¸ ì˜ë¯¸ì— ë§ëŠ” í•œêµ­ì–´ ê°ì„±/ìŠ¤íƒ€ì¼ ë‹¨ì–´ë¡œ ë°”ê¾¸ì–´ ì ìœ¼ì„¸ìš”.

ê·œì¹™:
- ì„¤ëª…ì€ ìµœëŒ€í•œ êµ¬ì²´ì ìœ¼ë¡œ, ê·¸ëŸ¬ë‚˜ ê³¼ì¥ ì—†ì´ ì‘ì„±í•©ë‹ˆë‹¤.
- ìƒ‰ê°/ë¬´ë“œ/ìŠ¤íƒ€ì¼ì€ ì‹¤ì œ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ê²ƒë§Œ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
- JSON ì´ì™¸ì˜ ë¬¸ì¥, ì„¤ëª…, ë§ˆí¬ë‹¤ìš´ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  ë¬¸ìì—´ì€ í°ë”°ì˜´í‘œ(\")ë¥¼ ì‚¬ìš©í•˜ê³ , JSON ë¬¸ë²•ì„ ì—„ê²©íˆ ì§€í‚¤ì„¸ìš”.
"""


@dataclass
class VLMResult:
    """VLM ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì´ì¬ ê°ì²´ë¡œ ê°ì‹¸ëŠ” í—¬í¼ (ì„ íƒì )."""
    image_type: str
    image_type_detail: str
    space_ko: str
    space_en: str
    style_keywords: List[str]
    color_keywords: List[str]
    mood_keywords: List[str]
    material_keywords: List[str]
    lighting_keywords: List[str]
    overall_comment_ko: str

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "VLMResult":
        return cls(
            image_type=str(data.get("image_type", "")),
            image_type_detail=str(data.get("image_type_detail", "")),
            space_ko=str(data.get("space_ko", "")),
            space_en=str(data.get("space_en", "")),
            style_keywords=list(data.get("style_keywords", [])),
            color_keywords=list(data.get("color_keywords", [])),
            mood_keywords=list(data.get("mood_keywords", [])),
            material_keywords=list(data.get("material_keywords", [])),
            lighting_keywords=list(data.get("lighting_keywords", [])),
            overall_comment_ko=str(data.get("overall_comment_ko", "")),
        )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "image_type": self.image_type,
            "image_type_detail": self.image_type_detail,
            "space_ko": self.space_ko,
            "space_en": self.space_en,
            "style_keywords": self.style_keywords,
            "color_keywords": self.color_keywords,
            "mood_keywords": self.mood_keywords,
            "material_keywords": self.material_keywords,
            "lighting_keywords": self.lighting_keywords,
            "overall_comment_ko": self.overall_comment_ko,
        }


# =========================
# í’ˆì§ˆ ì²´í¬ í—¬í¼
# =========================

def _check_image_quality(image_path: Union[str, Path]) -> Optional[str]:
    """
    - ë„ˆë¬´ íë¦° ì‚¬ì§„
    - ë„ˆë¬´ ì–´ë‘ìš´ ì‚¬ì§„
    - ë„ˆë¬´ ì‘ì€ ì‚¬ì§„
    ë“±ì„ ì‚¬ì „ì— ê±¸ëŸ¬ì„œ ë¬¸ì œ ìˆìœ¼ë©´ í•œêµ­ì–´ ì—ëŸ¬ ë©”ì‹œì§€ ë¬¸ìì—´ì„ ë°˜í™˜.
    ë¬¸ì œ ì—†ìœ¼ë©´ None ë°˜í™˜.
    """
    try:
        img = Image.open(image_path).convert("L")  # grayscale
    except Exception:
        return "ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ í˜•ì‹ì„ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”."

    arr = np.array(img, dtype=np.float32)

    h, w = arr.shape
    if h * w < 128 * 128:
        return "ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ì•„ì„œ ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì¢€ ë” í° ì‚¬ì´ì¦ˆì˜ ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    # ë°ê¸° ì²´í¬
    brightness = float(arr.mean())
    if brightness < 35:  # ë„ˆë¬´ ì–´ë‘ì›€
        return "ì‚¬ì§„ì´ ë„ˆë¬´ ì–´ë‘¡ìŠµë‹ˆë‹¤. ì¡°ëª…ì„ ì¼œê±°ë‚˜ ë°ì€ í™˜ê²½ì—ì„œ ì°ì€ ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    # íë¦¼(ë¸”ëŸ¬) ì²´í¬: ê°„ë‹¨í•œ ë¼í”Œë¼ì‹œì•ˆ ê¸°ë°˜
    # ì¤‘ì‹¬ í”½ì…€ - ì£¼ë³€ í”½ì…€ 4ê°œ í•© â†’ ë¼í”Œë¼ì‹œì•ˆ ê·¼ì‚¬
    center = arr[1:-1, 1:-1]
    lap = (
        arr[0:-2, 1:-1]
        + arr[2:, 1:-1]
        + arr[1:-1, 0:-2]
        + arr[1:-1, 2:]
        - 4 * center
    )
    blur_var = float(lap.var())

    # ê°’ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ íë¦° ì‚¬ì§„ì´ë¼ê³  íŒë‹¨
    if blur_var < 50.0:
        return "ì´ë¯¸ì§€ì˜ ë¶„ìœ„ê¸°ë¥¼ ì¸ì‹í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‚¬ì§„ì´ ë„ˆë¬´ íë¦¬ê²Œ ì°í˜€ ìˆì–´ìš”. ì¡°ê¸ˆ ë” ë°ê³  ì„ ëª…í•œ ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    return None


class QwenVLClient:
    """Qwen2.5-VL-7B-Instruct ë˜í¼. ëª¨ë¸/í”„ë¡œì„¸ì„œ 1íšŒ ë¡œë”©."""

    def __init__(
        self,
        model_name: str = VLM_MODEL_NAME,
        device: str = DEVICE,
        dtype: torch.dtype = DTYPE,
    ) -> None:
        self.model_name = model_name
        self.device = device

        # ëª¨ë¸ ë¡œë“œ
        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else None,
            attn_implementation="eager",  # flash_attn ì•ˆì“°ë©´ ê°€ì¥ ì•ˆì „
        )

        # í”„ë¡œì„¸ì„œ ë¡œë“œ (í•´ìƒë„ ì œí•œ ì„¤ì •)
        self.processor = AutoProcessor.from_pretrained(
            model_name,
            min_pixels=MIN_PIXELS,
            max_pixels=MAX_PIXELS,
        )

    # =========================
    # public: ì´ë¯¸ì§€ â†’ JSON
    # =========================
    def analyze_image(
        self,
        image_path: str | Path,
        user_hint: Optional[str] = None,
        max_new_tokens: int = 256,
        temperature: float = 0.2,
        top_p: float = 0.9,
    ) -> Dict[str, Any]:
        """
        ë°©/ì¸í…Œë¦¬ì–´ ì´ë¯¸ì§€ í•œ ì¥ì„ ë¶„ì„í•´ì„œ JSON dict ë¦¬í„´.

        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            user_hint: ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ì•Œë ¤ì£¼ëŠ” í…ìŠ¤íŠ¸(ì›í•˜ëŠ” ë¬´ë“œ/ì„¤ëª… ë“±), ì—†ì–´ë„ ë¨.
        """
        image_path = str(image_path)
        if not Path(image_path).is_file():
            raise FileNotFoundError(f"[VLM] ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

        # ğŸ”¹ 0) ê¸°ë³¸ í’ˆì§ˆ ì²´í¬ (ë„ˆë¬´ íë¦¼ / ì–´ë‘ì›€ / ì‘ì€ ì´ë¯¸ì§€)
        quality_msg = _check_image_quality(image_path)
        if quality_msg:
            # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì„ ê¹¨ì§€ ì•Šê¸° ìœ„í•´ JSON ìŠ¤í‚¤ë§ˆëŠ” ìœ ì§€í•˜ë˜,
            # overall_comment_ko ì— ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë„£ê³  ë‚˜ë¨¸ì§€ëŠ” ë¹„ì›Œì„œ ë°˜í™˜.
            return {
                "image_type": "invalid_quality",
                "image_type_detail": "",
                "space_ko": "",
                "space_en": "",
                "style_keywords": [],
                "color_keywords": [],
                "mood_keywords": [],
                "material_keywords": [],
                "lighting_keywords": [],
                "overall_comment_ko": quality_msg,
            }

        # ìœ ì € íŒíŠ¸ ë¬¸ì¥ êµ¬ì„±
        if user_hint:
            hint_text = (
                "ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” ì‚¬ìš©ìì˜ ì‹¤ì œ ë°©/ì¸í…Œë¦¬ì–´ ì‚¬ì§„ì¼ ìˆ˜ë„ ìˆê³  ì•„ë‹ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. "
                f"ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ë‚¨ê¸´ ì„¤ëª…: {user_hint!r}\n\n"
                "ì´ ì´ë¯¸ì§€ê°€ ì¸í…Œë¦¬ì–´ ê³µê°„ì¸ì§€, ì¸ë¬¼/ë°˜ë ¤ë™ë¬¼/í’ê²½/ì œí’ˆ í´ë¡œì¦ˆì—…ì¸ì§€ ë¨¼ì € íŒë‹¨í•œ ë’¤, "
                "ì•ì„œ ì„¤ëª…í•œ JSON ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."
            )
        else:
            hint_text = (
                "ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” ì‚¬ìš©ìì˜ ì‹¤ì œ ì‚¬ì§„ì…ë‹ˆë‹¤. "
                "ì´ ì´ë¯¸ì§€ê°€ ì¸í…Œë¦¬ì–´ ê³µê°„ì¸ì§€, ì¸ë¬¼/ë°˜ë ¤ë™ë¬¼/í’ê²½/ì œí’ˆ í´ë¡œì¦ˆì—…ì¸ì§€ ë¨¼ì € íŒë‹¨í•œ ë’¤, "
                "ì•ì„œ ì„¤ëª…í•œ JSON ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."
            )

        # Qwen2.5-VL ë©”ì‹œì§€ í¬ë§·
        messages = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": SYSTEM_PROMPT.strip()},
                ],
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_path,  # qwen_vl_utils ê°€ ê²½ë¡œë¥¼ ì½ì–´ì„œ ì²˜ë¦¬
                    },
                    {
                        "type": "text",
                        "text": hint_text,
                    },
                ],
            },
        ]

        # vision ì „ì²˜ë¦¬ (ì´ë¯¸ì§€/ë¹„ë””ì˜¤ í…ì„œ ì¤€ë¹„)
        image_inputs, video_inputs = process_vision_info(messages)

        # text + vision â†’ ëª¨ë¸ ì…ë ¥ í…ì„œ ìƒì„±
        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,  # assistant ì‘ë‹µ ìœ„ì¹˜ê¹Œì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        )

        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(self.device)

        # ìƒì„±
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
            )

        # prompt ë¶€ë¶„ì„ ì˜ë¼ë‚´ê³  assistant ì‘ë‹µë§Œ ë””ì½”ë”©
        input_ids = inputs["input_ids"]
        input_len = input_ids.shape[1]
        output_ids = generated_ids[:, input_len:]

        raw_text = self.processor.batch_decode(
            output_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )[0]

        # ğŸ” ì„ì‹œ ë””ë²„ê·¸: ëª¨ë¸ì´ ì‹¤ì œë¡œ ë­˜ ë±‰ëŠ”ì§€ ë³´ê¸°
        print("===== [VLM RAW OUTPUT] =====")
        print(raw_text)
        print("============================")

        # JSON íŒŒì‹±
        parsed = self._safe_parse_json(raw_text)

        # ğŸ”¹ 1ì°¨ ê²°ê³¼ë¥¼ VLMResultë¡œ ë˜í•‘
        result = VLMResult.from_dict(parsed)
        image_type = (result.image_type or "").strip().lower()

        # =========================
        # ì¸í…Œë¦¬ì–´ ê³µê°„ì´ ì•„ë‹Œ ê²½ìš° ì‚¬ì „ ì°¨ë‹¨
        # =========================
        if image_type and image_type != "interior_room":
            # ì´ë¯¸ì§€ ì¢…ë¥˜ë³„ ì•ˆë‚´ ë©”ì‹œì§€
            if image_type == "human":
                msg = "ê³µê°„ì´ ì•„ë‹Œ ì¸ë¬¼ ì‚¬ì§„ì´ë„¤ìš”. ì¸í…Œë¦¬ì–´ ê³µê°„ ì‚¬ì§„ì„ ì˜¬ë ¤ì£¼ì‹œë©´ ê·¸ì— ë§ê²Œ ì¶”ì²œë“œë¦´ê²Œìš”."
            elif image_type == "pet":
                msg = "ê³ ì–‘ì´Â·ê°•ì•„ì§€ ê°™ì€ ë°˜ë ¤ë™ë¬¼ ì¤‘ì‹¬ ì‚¬ì§„ì´ë¼ ë°© ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„í•˜ê¸´ ì–´ë ¤ì›Œìš”. ë°© ì „ì²´ê°€ ë³´ì´ëŠ” ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì˜¬ë ¤ì£¼ì„¸ìš”."
            elif image_type == "landscape":
                msg = "ì•¼ì™¸ í’ê²½ ì‚¬ì§„ì´ë¼ ì‹¤ë‚´ ì¸í…Œë¦¬ì–´ ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë°©ì´ë‚˜ ê±°ì‹¤ì²˜ëŸ¼ ì‹¤ë‚´ ê³µê°„ì´ ë³´ì´ëŠ” ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            elif image_type == "object_closeup":
                msg = "ì œí’ˆì´ë‚˜ ë¬¼ê±´ í•˜ë‚˜ë§Œ í¬ê²Œ ì°íŒ ì‚¬ì§„ì´ë¼ ê³µê°„ ì „ì²´ì˜ ë¬´ë“œë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë°© ì „ì²´ê°€ ì–´ëŠ ì •ë„ ë³´ì´ë„ë¡ ì°ì€ ì‚¬ì§„ì„ ì˜¬ë ¤ì£¼ì„¸ìš”."
            elif image_type == "document":
                msg = "ë¬¸ì„œÂ·í™”ë©´ ìº¡ì²˜ì²˜ëŸ¼ ë³´ì´ëŠ” ì´ë¯¸ì§€ë¼ ì¸í…Œë¦¬ì–´ ê³µê°„ ë¶„ì„ì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ìš”. ë°©ì´ë‚˜ ê±°ì‹¤ ì‚¬ì§„ì„ ì˜¬ë ¤ì£¼ì„¸ìš”."
            else:
                msg = "ë°© ì¸í…Œë¦¬ì–´ê°€ ì˜ ë³´ì´ì§€ ì•ŠëŠ” ì‚¬ì§„ì´ë¼ ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë°© ì „ì²´ê°€ ë³´ì´ëŠ” ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

            # ê¸°ì¡´ ìŠ¤í‚¤ë§ˆëŠ” ìœ ì§€í•˜ë˜, ì¶”ì²œ íŒŒì´í”„ë¼ì¸ì— ì˜í–¥ì´ ì—†ë„ë¡ ëŒ€ë¶€ë¶„ ë¹„ì›Œì„œ ë°˜í™˜
            return {
                "image_type": image_type,
                "image_type_detail": result.image_type_detail,
                "space_ko": "",
                "space_en": "",
                "style_keywords": [],
                "color_keywords": [],
                "mood_keywords": [],
                "material_keywords": [],
                "lighting_keywords": [],
                "overall_comment_ko": msg,
            }

        # =========================
        # ì¸í…Œë¦¬ì–´ ê³µê°„ìœ¼ë¡œ ì¸ì •ë˜ëŠ” ê²½ìš°: ê¸°ì¡´ ë™ì‘ ê·¸ëŒ€ë¡œ ìœ ì§€
        # =========================
        return result.to_dict()

    # =========================
    # ë‚´ë¶€ í—¬í¼
    # =========================
    @staticmethod
    def _safe_parse_json(text: str) -> Dict[str, Any]:
        """
        VLMì´ ì¶œë ¥í•œ í…ìŠ¤íŠ¸ì—ì„œ JSON ë¶€ë¶„ë§Œ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ ë½‘ì•„ë‚´ì„œ íŒŒì‹±í•œë‹¤.

        ì „ëµ:
        1) ```json ... ``` ì½”ë“œë¸”ë¡ ì•ˆì„ ë¨¼ì € ì‹œë„
        2) ì „ì²´ ë¬¸ìì—´ì—ì„œ { ... } ê· í˜•ì´ ë§ëŠ” ë©ì–´ë¦¬ë“¤ì„ ëª¨ë‘ ì°¾ì•„ì„œ,
           ê¸¸ì´ê°€ ê¸´ ê²ƒë¶€í„° í•˜ë‚˜ì”© json.loads ì‹œë„
        3) ê·¸ë˜ë„ ì•ˆ ë˜ë©´, ê¸°ì¡´ì²˜ëŸ¼ ì²« { ~ ë§ˆì§€ë§‰ } ë²”ìœ„ë¥¼ í•œ ë²ˆ ë” ì‹œë„
        4) ì™„ì „íˆ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ìŠ¤ì¼ˆë ˆí†¤ ë°˜í™˜
        """
        text = text.strip()

        def _try_parse(candidate: str) -> Optional[Dict[str, Any]]:
            candidate = candidate.strip()
            if not candidate:
                return None

            # 1ì°¨: ìˆëŠ” ê·¸ëŒ€ë¡œ
            try:
                return json.loads(candidate)
            except Exception:
                pass

            # 2ì°¨: ë„ˆë¬´ ì§€ì €ë¶„í•œ ë¬¸ìë§Œ ì œê±° (ì–¸ë”ìŠ¤ì½”ì–´ ë“±ì€ ìœ ì§€)
            try:
                cand2 = candidate.replace("\n", " ").replace("\r", " ")
                cand2 = re.sub(
                    r"[^\{\}\[\]0-9A-Za-zê°€-í£_\"\'\:\,\.\s\-]",
                    "",
                    cand2,
                )
                return json.loads(cand2)
            except Exception:
                return None

        # --------------------------------
        # 1) ```json ... ``` ë˜ëŠ” ``` ... ``` ì½”ë“œë¸”ë¡ ìš°ì„  ì‹œë„
        # --------------------------------
        fence = re.search(r"```json([\s\S]*?)```", text, re.IGNORECASE)
        if not fence:
            fence = re.search(r"```([\s\S]*?)```", text)

        if fence:
            block = fence.group(1)
            parsed = _try_parse(block)
            if parsed is not None:
                return parsed

        # --------------------------------
        # 2) ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ { ... } ê· í˜• ì¡íŒ ë©ì–´ë¦¬ë“¤ ëª¨ë‘ ì¶”ì¶œ
        #    (ìŠ¤íƒìœ¼ë¡œ ë°”ê¹¥/ì•ˆìª½ ì¤‘ê´„í˜¸ ë§¤ì¹­)
        # --------------------------------
        candidates: List[str] = []
        stack = []
        start_idx: Optional[int] = None

        for i, ch in enumerate(text):
            if ch == "{":
                if not stack:
                    start_idx = i
                stack.append("{")
            elif ch == "}":
                if stack:
                    stack.pop()
                    if not stack and start_idx is not None:
                        candidates.append(text[start_idx:i + 1])
                        start_idx = None

        # ê¸¸ì´ ê¸´ ê²ƒ(=ë°”ê¹¥ JSONì¼ ê°€ëŠ¥ì„±)ì´ ë¨¼ì € ê°€ë„ë¡ ì •ë ¬
        for cand in sorted(candidates, key=len, reverse=True):
            parsed = _try_parse(cand)
            if parsed is not None:
                return parsed

        # --------------------------------
        # 3) ë§ˆì§€ë§‰ fallback: ì²« { ~ ë§ˆì§€ë§‰ } ë²”ìœ„ í•œ ë²ˆ ë” ì‹œë„
        # --------------------------------
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            cand = text[start:end + 1]
            parsed = _try_parse(cand)
            if parsed is not None:
                return parsed

        # --------------------------------
        # 4) ì™„ì „ ì‹¤íŒ¨ â†’ ê¸°ë³¸ ìŠ¤ì¼ˆë ˆí†¤
        # --------------------------------
        return {
            "image_type": "",
            "image_type_detail": "",
            "space_ko": "",
            "space_en": "",
            "style_keywords": [],
            "color_keywords": [],
            "mood_keywords": [],
            "material_keywords": [],
            "lighting_keywords": [],
            "overall_comment_ko": "",
        }




# ì‹±ê¸€í†¤ í˜•íƒœë¡œ ì¬ì‚¬ìš© (Streamlit / CLI ì–‘ìª½ì—ì„œ ê³µìš©ìœ¼ë¡œ ì“°ê¸° í¸í•˜ê²Œ)
_vlm_client: Optional[QwenVLClient] = None


def get_vlm_client() -> QwenVLClient:
    global _vlm_client
    if _vlm_client is None:
        _vlm_client = QwenVLClient()
    return _vlm_client


def analyze_room_image(
    image_path: str | Path,
    user_hint: Optional[str] = None,
) -> Dict[str, Any]:
    """
    ì™¸ë¶€ì—ì„œ ì£¼ë¡œ í˜¸ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜.

    ì˜ˆ)
        from input_vlm import analyze_room_image
        result = analyze_room_image("examples/room.jpg", "ì›ëª©ê°€êµ¬ ìœ„ì£¼, ë”°ëœ»í•œ ë¶„ìœ„ê¸° ì¢‹ì•„í•¨")
    """
    client = get_vlm_client()
    return client.analyze_image(image_path, user_hint=user_hint)


# ============================================================
#  VLM ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ë°”ë¡œ ì“°ê¸° ì¢‹ì€ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” í—¬í¼
# ============================================================

def _normalize_str_list(val: Any) -> List[str]:
    """
    ë¬¸ìì—´ / ë¦¬ìŠ¤íŠ¸ / íŠœí”Œ í˜•íƒœë¡œ ì˜¬ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë¥¼
    ['í‚¤ì›Œë“œ1', 'í‚¤ì›Œë“œ2', ...] í˜•íƒœë¡œ ì •ê·œí™”
    """
    if isinstance(val, str):
        # ì‰¼í‘œ/ìŠ¬ë˜ì‹œ/ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        chunks = re.split(r"[,\n/]", val)
        return [c.strip() for c in chunks if c.strip()]
    elif isinstance(val, (list, tuple)):
        return [str(x).strip() for x in val if str(x).strip()]
    else:
        return []


def infer_state_from_room_image(
    image_path: str | Path,
    user_hint: Optional[str] = None,
) -> Dict[str, Any]:
    """
    âœ… main.pyì—ì„œ ë°”ë¡œ ChatStateë¡œ ì˜®ê²¨ ë‹´ê¸° ì¢‹ì€ í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” í—¬í¼.

    ë°˜í™˜ í˜•ì‹:
        {
            "space": "ì¹¨ì‹¤" ë˜ëŠ” "ê±°ì‹¤" ë“± (ì—†ìœ¼ë©´ None),
            "moods": [...],
            "style_keywords": [...],
            "color_keywords": [...],
            "material_keywords": [...],
            "lighting_keywords": [...],
            "raw": VLM ì›ë³¸ dict
        }
    """
    raw = analyze_room_image(image_path, user_hint=user_hint)

    space = raw.get("space_ko") or raw.get("space_en") or None

    moods = _normalize_str_list(raw.get("mood_keywords", []))
    style_keywords = _normalize_str_list(raw.get("style_keywords", []))
    color_keywords = _normalize_str_list(raw.get("color_keywords", []))
    material_keywords = _normalize_str_list(raw.get("material_keywords", []))
    lighting_keywords = _normalize_str_list(raw.get("lighting_keywords", []))

    return {
        "space": space if space else None,
        "moods": moods,
        "style_keywords": style_keywords,
        "color_keywords": color_keywords,
        "material_keywords": material_keywords,
        "lighting_keywords": lighting_keywords,
        "raw": raw,
    }


def infer_mood_from_room_image(
    image_path: str | Path,
    user_hint: Optional[str] = None,
):
    """
    âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ìš© ë˜í¼.

    - ë‚´ë¶€ì ìœ¼ë¡œ infer_state_from_room_image()ë¥¼ í˜¸ì¶œí•´ì„œ
      ì „ì²´ ì •ë³´ë¥¼ ì–»ê³ ,
    - ê·¸ ì¤‘ì—ì„œ mood_keywordsë§Œ ë½‘ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.
    """
    info = infer_state_from_room_image(image_path, user_hint=user_hint)
    moods = info.get("moods") or []

    if isinstance(moods, str):
        moods = [m.strip() for m in moods.split(",") if m.strip()]
    elif isinstance(moods, (list, tuple)):
        moods = [str(m).strip() for m in moods if str(m).strip()]
    else:
        moods = []

    return moods


# ê°„ë‹¨ ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    import argparse
    from pprint import pprint

    parser = argparse.ArgumentParser()
    parser.add_argument("image", type=str, help="ë¶„ì„í•  ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument(
        "--hint",
        type=str,
        default=None,
        help="ì‚¬ìš©ì íŒíŠ¸ í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)",
    )
    args = parser.parse_args()

    print(f"[VLM] ëª¨ë¸: {VLM_MODEL_NAME}")
    print(f"[VLM] ì´ë¯¸ì§€: {args.image}")
    result_dict = analyze_room_image(args.image, user_hint=args.hint)
    pprint(result_dict, width=120)
