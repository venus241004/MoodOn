# rag_index.py
"""
products_all_ver1_vlm.json â†’ Chroma VectorDB ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ (OpenAI Embedding ë²„ì „)

- í•œ ìƒí’ˆë‹¹ í•˜ë‚˜ì˜ document
- ë¬´ë“œ í‚¤ì›Œë“œ / ì¹´í…Œê³ ë¦¬ / ê°€ê²© / ë¸Œëžœë“œ ë“± ë©”íƒ€ë°ì´í„° ì €ìž¥
- ìž„ë² ë”©: OpenAI text-embedding-3-large (config.EMBEDDING_MODEL_NAME)

âš ï¸ ì£¼ì˜:
  - build_vector_db.pyì™€ ë™ì¼í•˜ê²Œ 'products' ì»¬ë ‰ì…˜ì„ ìƒì„±í•œë‹¤.
  - ì´ íŒŒì¼ì„ ì‹¤í–‰í•˜ë©´ ê¸°ì¡´ 'products' ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ë§Œë“ ë‹¤.
"""

import json
from pathlib import Path
from typing import List, Dict, Any

import chromadb
from chromadb.config import Settings
from openai import OpenAI

from config import (
    PRODUCTS_JSON_PATH,
    VECTOR_DB_DIR,
    EMBEDDING_MODEL_NAME,
)

# ðŸ”¹ ë¬´ë“œ ì •ê·œí™” ìœ í‹¸ (mood_vocab.py)
from mood_vocab import snap_moods_to_vocab


# =========================
# 0. OpenAI Embedding í—¬í¼
# =========================

_client: OpenAI | None = None


def _get_client() -> OpenAI:
    global _client
    if _client is None:
        _client = OpenAI()  # OPENAI_API_KEYëŠ” .env / í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ
    return _client


def embed_texts(texts: List[str]) -> List[List[float]]:
    """
    OpenAI text-embedding-3-largeë¡œ ì—¬ëŸ¬ ë¬¸ìž¥ì„ ìž„ë² ë”©.
    """
    client = _get_client()
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL_NAME,
        input=texts,
    )
    return [d.embedding for d in resp.data]


# =========================
# 1. ì¸ë±ìŠ¤ ë¹Œë”
# =========================

COLLECTION_NAME = "products"


def build_index():
    print("â–¶ RAG ì¸ë±ì‹± ì‹œìž‘ (OpenAI Embeddings)")
    print(f"  - JSON: {PRODUCTS_JSON_PATH}")
    print(f"  - Vector DB: {VECTOR_DB_DIR}")
    print(f"  - EMBEDDING_MODEL: {EMBEDDING_MODEL_NAME}")

    # ê²½ë¡œ ìƒì„±
    VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)

    # Chroma í´ë¼ì´ì–¸íŠ¸
    client = chromadb.PersistentClient(
        path=str(VECTOR_DB_DIR),
        settings=Settings(anonymized_telemetry=False),
    )

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìž¬ìƒì„±
    try:
        client.delete_collection(COLLECTION_NAME)
        print(f"  - ê¸°ì¡´ '{COLLECTION_NAME}' ì»¬ë ‰ì…˜ ì‚­ì œ")
    except Exception:
        print(f"  - ê¸°ì¡´ '{COLLECTION_NAME}' ì»¬ë ‰ì…˜ ì—†ìŒ (ë¬´ì‹œ)")

    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"},
    )

    # JSON ë¡œë“œ
    with open(PRODUCTS_JSON_PATH, "r", encoding="utf-8") as f:
        products: List[Dict[str, Any]] = json.load(f)

    print(f"ðŸ“¦ ì´ ìƒí’ˆ ìˆ˜(ì›ë³¸): {len(products)}ê°œ")

    ids: List[str] = []
    docs: List[str] = []
    metadatas: List[Dict[str, Any]] = []

    seen_ids = set()
    skipped_duplicates = 0

    for p in products:
        product_id = p.get("product_id")
        if not product_id:
            # product_idê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ì•ˆì •ì„±ìš©)
            continue

        # ðŸ”¹ ë™ì¼ product_idê°€ ì´ë¯¸ ë“¤ì–´ê°€ ìžˆìœ¼ë©´ ìŠ¤í‚µ
        if product_id in seen_ids:
            skipped_duplicates += 1
            # í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ë””ë²„ê·¸ ì¶œë ¥ ê°€ëŠ¥:
            # print(f"  - ì¤‘ë³µ product_id ë°œê²¬, ìŠ¤í‚µ: {product_id}")
            continue

        seen_ids.add(product_id)

        category_id = p.get("category_id", "")
        brand_name = p.get("brand_name", "")
        product_name = p.get("product_name", "")
        price_str = p.get("price", "0")

        # ---- ê°€ê²© íŒŒì‹± ----
        try:
            price_int = int(price_str)
        except Exception:
            price_int = 0

        # ---- ë¬´ë“œ ì²˜ë¦¬: raw â†’ ì •ê·œí™”(vocab) ----
        raw_moods = p.get("mood_keywords", []) or p.get("moods", []) or []

        if isinstance(raw_moods, str):
            raw_mood_list = [m.strip() for m in raw_moods.split(",") if m.strip()]
        elif isinstance(raw_moods, list):
            raw_mood_list = [str(m).strip() for m in raw_moods if str(m).strip()]
        else:
            raw_mood_list = []

        # ðŸ”¹ vocabì— ë§žì¶° ì •ê·œí™” (ëŒ€í‘œ ë¬´ë“œë¡œ ìŠ¤ëƒ…)
        canonical_moods, unknown_moods = snap_moods_to_vocab(raw_mood_list)

        # doc_textì— ë„£ì„ ë¬´ë“œ í…ìŠ¤íŠ¸: ê°€ëŠ¥í•˜ë©´ ì •ê·œí™”ëœ ë¬´ë“œ ì‚¬ìš©
        moods_for_text = canonical_moods or raw_mood_list
        if moods_for_text:
            moods_str = ", ".join(moods_for_text)
        else:
            moods_str = ""

        # ë©”íƒ€ë°ì´í„°ìš© ë¬¸ìžì—´ ë²„ì „ë“¤ (ChromaëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ í—ˆìš©í•˜ì§€ ì•ŠìŒ)
        canonical_moods_str = ", ".join(canonical_moods) if canonical_moods else ""
        raw_moods_str = ", ".join(raw_mood_list) if raw_mood_list else ""
        unknown_moods_str = ", ".join(unknown_moods) if unknown_moods else ""

        # ìž„ë² ë”©ìš© í…ìŠ¤íŠ¸ êµ¬ì„±
        text_parts = [
            f"[ì¹´í…Œê³ ë¦¬] {category_id}",
            f"[ë¸Œëžœë“œ] {brand_name}",
            f"[ìƒí’ˆëª…] {product_name}",
            f"[ê°€ê²©] {price_str}ì›",
        ]
        if moods_str:
            text_parts.append("[ë¬´ë“œ í‚¤ì›Œë“œ] " + moods_str)

        doc_text = "\n".join(text_parts)

        ids.append(product_id)
        docs.append(doc_text)
        metadatas.append(
            {
                "product_id": product_id,
                "category_id": category_id,
                "brand_name": brand_name,
                "price": price_int,
                # ðŸ”¹ RAG ê²€ìƒ‰ì—ì„œ ì‚¬ìš©í•  í‘œì¤€í™”ëœ ë¬´ë“œ (ë¬¸ìžì—´)
                "mood_keywords": canonical_moods_str or raw_moods_str,
                "mood_keywords_count": len(canonical_moods or raw_mood_list),
                # ðŸ”¹ ì›ë³¸ JSONì— ìžˆë˜ ë¬´ë“œ (ë¡œìš° ë°ì´í„° ë³´ì¡´, ë¬¸ìžì—´)
                "raw_mood_keywords": raw_moods_str,
                # ðŸ”¹ vocabì— ë§¤ì¹­ë˜ì§€ ì•Šì€ ë¬´ë“œë“¤(ë¶„ì„/ë””ë²„ê¹…ìš©, ë¬¸ìžì—´)
                "unknown_mood_keywords": unknown_moods_str,
                "link_url": p.get("link_url", ""),
                "image_url": p.get("image_url", ""),
                "s3_path": p.get("s3_path", ""),
                "s3_url": p.get("s3_url", ""),
                "mood_category": p.get("mood_category", ""),
                "source_site": infer_source_site(product_id),
            }
        )

    print(f"âœ… ì¤‘ë³µ ì œê±° í›„ ì‹¤ì œ ì¸ë±ì‹± ëŒ€ìƒ ìƒí’ˆ ìˆ˜: {len(ids)}ê°œ")
    if skipped_duplicates > 0:
        print(f"  - ì¤‘ë³µ product_idë¡œ ì¸í•´ ìŠ¤í‚µëœ ê°œìˆ˜: {skipped_duplicates}ê°œ")

    print("ðŸ§  ìž„ë² ë”© ê³„ì‚° ì¤‘... (OpenAI API)")
    embeddings: List[List[float]] = []
    BATCH_SIZE = 128
    total = len(docs)

    for i in range(0, total, BATCH_SIZE):
        batch_docs = docs[i : i + BATCH_SIZE]
        batch_embs = embed_texts(batch_docs)
        embeddings.extend(batch_embs)
        print(f"  - {i + len(batch_docs)}/{total}ê°œ ì™„ë£Œ")

    print("ðŸ’¾ Chroma ì»¬ë ‰ì…˜ì— ì¶”ê°€ ì¤‘...")
    collection.add(
        ids=ids,
        documents=docs,
        embeddings=embeddings,
        metadatas=metadatas,
    )

    print("âœ… ì¸ë±ì‹± ì™„ë£Œ!")


def infer_source_site(product_id: str) -> str:
    """
    product_id íŒ¨í„´ìœ¼ë¡œ ê°„ë‹¨ížˆ ì¶œì²˜ ë¶„ë¥˜
    ì˜ˆ: ten_..., kakao_..., guud_...
    """
    if not isinstance(product_id, str):
        return "unknown"

    if product_id.startswith("ten_"):
        return "10x10"
    if product_id.startswith("kakao_"):
        return "kakao"
    if product_id.startswith("guud_"):
        return "guud"
    return "unknown"


if __name__ == "__main__":
    build_index()
